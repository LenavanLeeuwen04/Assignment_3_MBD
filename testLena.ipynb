{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ddc71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mesa import Agent, Model\n",
    "from mesa.time import SimultaneousActivation\n",
    "from mesa.space import NetworkGrid\n",
    "from mesa.datacollection import DataCollector\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Iterable, List, Dict\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9518b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Strategy selection helpers\n",
    "#\n",
    "# We provide two different ways agents can choose their strategy:\n",
    "# - `choose_strategy_imitate`: choose strategy of the highest-payoff neighbour (including self).\n",
    "# - `choose_strategy_logit`: choose strategy using logit / softmax choice.\n",
    "#\n",
    "####################################\n",
    "def choose_strategy_imitate(agent, neighbors):\n",
    "    \"\"\"Choose strategy of the highest-payoff neighbour (including self).\"\"\"\n",
    "    candidates = neighbors + [agent]\n",
    "    best = max(candidates, key=lambda a: a.payoff)\n",
    "    return best.strategy\n",
    "\n",
    "def choose_strategy_logit(agent, neighbors, a_I, b, tau):\n",
    "    \"\"\"Choose strategy using logit / softmax choice.\n",
    "\n",
    "    Parameters\n",
    "    - agent: the agent choosing a strategy\n",
    "    - neighbors: list of neighbour agents\n",
    "    - a_I: effective coordination payoff given current infrastructure\n",
    "    - b: defection payoff\n",
    "    - tau: temperature parameter for softmax\n",
    "    \"\"\"\n",
    "    # compute expected payoffs for C and D\n",
    "    pi_C = 0.0\n",
    "    pi_D = 0.0\n",
    "    for other in neighbors:\n",
    "        s_j = other.strategy\n",
    "        if s_j == \"C\":\n",
    "            pi_C += a_I\n",
    "            pi_D += b\n",
    "        else:\n",
    "            pi_C += 0.0\n",
    "            pi_D += b\n",
    "\n",
    "    # softmax choice\n",
    "    denom = np.exp(pi_C / tau) + np.exp(pi_D / tau)\n",
    "    P_C = np.exp(pi_C / tau) / denom if denom > 0 else 0.5\n",
    "    return \"C\" if random.random() < P_C else \"D\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de071048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################\n",
    "# Agent class\n",
    "#\n",
    "# The EVAgent class implements the single agent at a graph node.\n",
    "#\n",
    "# Attributes\n",
    "# - strategy: \"C\" (adopt EV) or \"D\" (defect / ICE)\n",
    "# - payoff: accumulated payoff from interactions with neighbours\n",
    "# - next_strategy: strategy chosen for the next time step\n",
    "####################################\n",
    "\n",
    "def __init__(self, unique_id, model, init_strategy=\"D\"):\n",
    "    super().__init__(unique_id, model)\n",
    "    self.strategy = init_strategy\n",
    "    self.payoff = 0.0\n",
    "    self.next_strategy = init_strategy\n",
    "\n",
    "def step(self):\n",
    "    \"\"\"Compute payoff from interactions with neighbours and choose next_strategy.\n",
    "\n",
    "    Stag Hunt payoff rules:\n",
    "    - C vs C: `a_I` (coordination enhanced by infrastructure)\n",
    "    - C vs D: 0\n",
    "    - D vs C: `b`\n",
    "    - D vs D: `b`\n",
    "    \"\"\"\n",
    "    I = self.model.infrastructure\n",
    "    a0 = self.model.a0\n",
    "    beta_I = self.model.beta_I\n",
    "    b = self.model.b\n",
    "    a_I = a0 + beta_I * I\n",
    "\n",
    "    neighbor_agents = []\n",
    "    for nbr in self.model.G.neighbors(self.pos):\n",
    "        neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "    if not neighbor_agents:\n",
    "        self.payoff = 0.0\n",
    "        # still set next_strategy deterministically to current strategy\n",
    "        self.next_strategy = self.strategy\n",
    "        return\n",
    "\n",
    "    payoff = 0.0\n",
    "    for other in neighbor_agents:\n",
    "        s_i = self.strategy\n",
    "        s_j = other.strategy\n",
    "        if s_i == \"C\" and s_j == \"C\":\n",
    "            payoff += a_I\n",
    "        elif s_i == \"C\" and s_j == \"D\":\n",
    "            payoff += 0.0\n",
    "        elif s_i == \"D\" and s_j == \"C\":\n",
    "            payoff += b\n",
    "        else:\n",
    "            payoff += b\n",
    "    self.payoff = payoff\n",
    "\n",
    "# --- Strategy computation moved INTO step ---\n",
    "# This ensures SimultaneousActivation will call step() for all agents\n",
    "# to compute next_strategy, and advance() below only commits it.\n",
    "    func = getattr(self.model, \"strategy_choice_func\", \"imitate\")\n",
    "\n",
    "    if func == \"imitate\":\n",
    "        self.next_strategy = choose_strategy_imitate(self, neighbor_agents)\n",
    "    elif func == \"logit\":\n",
    "        tau = getattr(self.model, \"tau\", 1.0)\n",
    "        self.next_strategy = choose_strategy_logit(self, neighbor_agents, a_I, b, tau)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy choice function: {func}\")\n",
    "    \n",
    "    def advance(self, strategy_choice_func=\"imitate\"):\n",
    "        \"\"\"Commit next_strategy for synchronous updates.\n",
    "\n",
    "        The selection of next_strategy is performed in `step()` so that\n",
    "        `SimultaneousActivation` works correctly: all agents compute their\n",
    "        next_strategy in step(), then all agents commit them in advance().\n",
    "        \"\"\"\n",
    "# Only commit the choice here (do not recompute).\n",
    "        self.strategy = self.next_strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ####################################\n",
    "    # Advance method\n",
    "    #\n",
    "    # The advance method updates the agent's strategy based on the selected rule.\n",
    "    #\n",
    "    # Parameters\n",
    "    # - strategy_choice_func: the strategy selection function to use (\"imitate\" or \"logit\")\n",
    "    ####################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1209379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ####################################\n",
    "    # Model class\n",
    "    #\n",
    "    # The EVStagHuntModel class implements the Mesa model for EV Stag Hunt on a network.\n",
    "    #\n",
    "    # Parameters\n",
    "    # - initial_ev: number of initial EV nodes\n",
    "    # - a0: base payoff for EV adoption\n",
    "    # - beta_I: payoff enhancement factor for EV adoption\n",
    "    # - b: payoff for ICE defection\n",
    "    # - g_I: infrastructure growth rate\n",
    "    # - I0: initial infrastructure level\n",
    "    # - seed: random seed for reproducibility\n",
    "    # - network_type: type of network to generate (\"random\" or \"BA\")\n",
    "    # - n_nodes: number of nodes in the network\n",
    "    # - p: probability of edge creation in random network\n",
    "    # - m: number of edges to attach to new node in BA network\n",
    "    # - collect: whether to collect agent and model-level data\n",
    "    # - strategy_choice_func: strategy selection function (\"imitate\" or \"logit\")\n",
    "    # - tau: temperature parameter for softmax choice (only used with \"logit\")\n",
    "    ####################################\n",
    "#\n",
    "class EVStagHuntModel(Model):\n",
    "    \"\"\"Mesa model for EV Stag Hunt on a network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_ev=10,\n",
    "        a0=2.0,\n",
    "        beta_I=3.0,\n",
    "        b=1.0,\n",
    "        g_I=0.1,\n",
    "        I0=0.05,\n",
    "        seed=None,\n",
    "        network_type=\"random\",\n",
    "        n_nodes=100,\n",
    "        p=0.05,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        strategy_choice_func: str = \"imitate\",\n",
    "        tau: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(seed=seed)\n",
    "\n",
    "        # Build graph\n",
    "        if network_type == \"BA\":\n",
    "            G = nx.barabasi_albert_graph(n_nodes, m, seed=seed)\n",
    "        else:\n",
    "            G = nx.erdos_renyi_graph(n_nodes, p, seed=seed)\n",
    "        self.G = G\n",
    "        self.grid = NetworkGrid(G)\n",
    "        self.schedule = SimultaneousActivation(self)\n",
    "\n",
    "        # parameters\n",
    "        self.a0 = a0\n",
    "        self.beta_I = beta_I\n",
    "        self.b = b\n",
    "        self.g_I = g_I\n",
    "        self.infrastructure = I0\n",
    "        self.step_count = 0\n",
    "        self.strategy_choice_func = strategy_choice_func\n",
    "        self.tau = tau\n",
    "\n",
    "        # initialize node attribute for agent reference\n",
    "        for n in self.G.nodes:\n",
    "            self.G.nodes[n][\"agent\"] = []\n",
    "\n",
    "        # choose initial EV nodes\n",
    "        total_nodes = self.G.number_of_nodes()\n",
    "        k_ev = max(0, min(initial_ev, total_nodes))\n",
    "        ev_nodes = set(self.random.sample(list(self.G.nodes), k_ev))\n",
    "\n",
    "        # create one agent per node\n",
    "        uid = 0\n",
    "        for node in self.G.nodes:\n",
    "            init_strategy = \"C\" if node in ev_nodes else \"D\"\n",
    "            agent = EVAgent(uid, self, init_strategy)\n",
    "            uid += 1\n",
    "            self.schedule.add(agent)\n",
    "            self.grid.place_agent(agent, node)\n",
    "\n",
    "        self.datacollector = None\n",
    "        if collect:\n",
    "            self.datacollector = DataCollector(\n",
    "                model_reporters={\n",
    "                    \"X\": self.get_adoption_fraction,\n",
    "                    \"I\": lambda m: m.infrastructure,\n",
    "                },\n",
    "                agent_reporters={\"strategy\": \"strategy\", \"payoff\": \"payoff\"},\n",
    "            )\n",
    "\n",
    "    def get_adoption_fraction(self):\n",
    "        agents = self.schedule.agents\n",
    "        if not agents:\n",
    "            return 0.0\n",
    "        return sum(1 for a in agents if a.strategy == \"C\") / len(agents)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5fad4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "    # ####################\n",
    "    # Model step function\n",
    "    #\n",
    "    # The step function advances the model by one time step.\n",
    "    # It first advances all agents, then computes the adoption fraction and infrastructure level.\n",
    "    # The infrastructure level is updated based on the adoption fraction and the infrastructure growth rate.\n",
    "    # The updated infrastructure level is clipped to the interval [0, 1].\n",
    "    # Finally, if data collection is enabled, the model and agent data are collected.\n",
    "    #######################\n",
    "def step(self): \n",
    "        self.schedule.step() # advance all agents\n",
    "        X = self.get_adoption_fraction() # compute adoption fraction after all agents have advanced\n",
    "        I = self.infrastructure # infrastructure level before this step\n",
    "        dI = self.g_I * (X - I) # infrastructure growth rate, impacted by adoption fraction\n",
    "        self.infrastructure = float(min(1.0, max(0.0, I + dI))) # clip infrastructure level to [0, 1]\n",
    "        if self.datacollector is not None:\n",
    "            self.datacollector.collect(self) # collect data at the end of each step\n",
    "        self.step_count += 1 # increment step count after data collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d403b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################\n",
    "#\n",
    "# Set initial adopters\n",
    "# \n",
    "# Parameters\n",
    "# - model: the EVStagHuntModel instance\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - method: method to choose initial adopters (\"random\" or \"degree\")\n",
    "# - seed: random seed for reproducibility\n",
    "# - high: whether to choose high or low degree nodes for \"degree\" method\n",
    "###########################\n",
    "def set_initial_adopters(model, X0_frac, method=\"random\", seed=None, high=True):\n",
    "    \"\"\"Set a fraction of agents to EV adopters using different heuristics.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    agents = model.schedule.agents\n",
    "    n = len(agents)\n",
    "    k = int(round(X0_frac * n))\n",
    "\n",
    "    for a in agents:\n",
    "        a.strategy = \"D\"\n",
    "\n",
    "    if k <= 0:\n",
    "        return\n",
    "\n",
    "    if method == \"random\":\n",
    "        idx = rng.choice(n, size=k, replace=False)\n",
    "        for i in idx:\n",
    "            agents[i].strategy = \"C\"\n",
    "        return\n",
    "\n",
    "    if method == \"degree\":\n",
    "        deg = dict(model.G.degree())\n",
    "        ordered_nodes = sorted(deg.keys(), key=lambda u: deg[u], reverse=high)\n",
    "        chosen = set(ordered_nodes[:k])\n",
    "        for a in agents:\n",
    "            if a.unique_id in chosen:\n",
    "                a.strategy = \"C\"\n",
    "        return\n",
    "\n",
    "    raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae041a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Ratio sweep helpers (computation-only)\n",
    "# -----------------------------\n",
    "#########################\n",
    "#\n",
    "# Run a single network trial\n",
    "# \n",
    "# Parameters\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - ratio: payoff ratio between EV and DC agents (a0 = ratio*b - beta_I*I0)\n",
    "# - I0: initial infrastructure level\n",
    "# - beta_I: cost of EV adoption relative to DC (beta_I*I0)\n",
    "# - b: payoff of EV (b)\n",
    "# - g_I: infrastructure growth rate (g_I)\n",
    "# - T: number of time steps to run\n",
    "# - network_type: type of network to generate (\"random\" or \"BA\")\n",
    "# - n_nodes: number of nodes in the network\n",
    "# - p: probability of edge creation in random network\n",
    "# - m: number of edges to attach from a new node to existing nodes in BA network\n",
    "# - seed: random seed for reproducibility\n",
    "# - tol: tolerance for convergence check (default: 1e-3)\n",
    "# - patience: number of steps to wait for convergence (default: 30)\n",
    "\n",
    "def run_network_trial(\n",
    "    X0_frac: float,\n",
    "    ratio: float,\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    seed: int | None = None,\n",
    "    tol: float = 1e-3,\n",
    "    patience: int = 30,\n",
    "    collect: bool = False,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"Run a single realisation and return final adoption fraction.\n",
    "\n",
    "    Preserves the intended initial payoff ratio via a0 = ratio*b - beta_I*I0.\n",
    "    Includes basic stability-based early stopping.\n",
    "    \"\"\"\n",
    "    initial_ev = int(round(X0_frac * n_nodes))\n",
    "    a0 = ratio * b - beta_I * I0\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        initial_ev=initial_ev,\n",
    "        a0=a0,\n",
    "        beta_I=beta_I,\n",
    "        b=b,\n",
    "        g_I=g_I,\n",
    "        I0=I0,\n",
    "        seed=seed,\n",
    "        network_type=network_type,\n",
    "        n_nodes=n_nodes,\n",
    "        p=p,\n",
    "        m=m,\n",
    "        collect=collect,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    stable_steps = 0\n",
    "    prev_X = None\n",
    "    prev_I = None\n",
    "    for _ in range(T):\n",
    "        model.step()\n",
    "        X = model.get_adoption_fraction()\n",
    "        I = model.infrastructure\n",
    "        if prev_X is not None and prev_I is not None:\n",
    "            if abs(X - prev_X) < tol and abs(I - prev_I) < tol:\n",
    "                stable_steps += 1\n",
    "            else:\n",
    "                stable_steps = 0\n",
    "        prev_X, prev_I = X, I\n",
    "        if X in (0.0, 1.0) and stable_steps >= 10:\n",
    "            break\n",
    "        if stable_steps >= patience:\n",
    "            break\n",
    "\n",
    "    return model.get_adoption_fraction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db1553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################\n",
    "#\n",
    "# Compute final mean adoption fraction vs ratio\n",
    "# \n",
    "##########################\n",
    "def final_mean_adoption_vs_ratio(\n",
    "    X0_frac: float,\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute mean final adoption across a sweep of ratio values.\n",
    "\n",
    "    For each ratio, average over `batch_size` trials with jittered `I0` and seeds.\n",
    "    Returns a numpy array of means aligned with `ratio_values` order.\n",
    "    \"\"\"\n",
    "    ratios = list(ratio_values)\n",
    "    means: List[float] = []\n",
    "    for ratio in ratios:\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        means.append(float(np.mean(finals)))\n",
    "    return np.asarray(means, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5974b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################\n",
    "#\n",
    "# Compute heatmap row for a fixed ratio\n",
    "# \n",
    "##########################\n",
    "def _row_for_ratio_task(args: Dict) -> np.ndarray:\n",
    "    \"\"\"Top-level worker to compute one heatmap row for a fixed ratio.\n",
    "\n",
    "    Returns an array of mean final adoption across provided X0_values.\n",
    "    \"\"\"\n",
    "    ratio = args[\"ratio\"]\n",
    "    X0_values = args[\"X0_values\"]\n",
    "    I0 = args[\"I0\"]\n",
    "    beta_I = args[\"beta_I\"]\n",
    "    b = args[\"b\"]\n",
    "    g_I = args[\"g_I\"]\n",
    "    T = args[\"T\"]\n",
    "    network_type = args[\"network_type\"]\n",
    "    n_nodes = args[\"n_nodes\"]\n",
    "    p = args[\"p\"]\n",
    "    m = args[\"m\"]\n",
    "    batch_size = args[\"batch_size\"]\n",
    "    init_noise_I = args[\"init_noise_I\"]\n",
    "    strategy_choice_func = args[\"strategy_choice_func\"]\n",
    "    tau = args[\"tau\"]\n",
    "\n",
    "    row = np.empty(len(X0_values), dtype=float)\n",
    "    for j, X0 in enumerate(X0_values):\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac=X0,\n",
    "                ratio=ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        row[j] = float(np.mean(finals))\n",
    "    return row\n",
    "\n",
    "    \n",
    "#########################\n",
    "#\n",
    "# Compute heatmap matrix for phase sweep\n",
    "# \n",
    "##########################\n",
    "def phase_sweep_X0_vs_ratio(\n",
    "    X0_values: Iterable[float],\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 250,\n",
    "    network_type: str = \"BA\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute a heatmap matrix of mean final adoption X* over (X0, ratio).\n",
    "\n",
    "    Returns an array of shape (len(ratio_values), len(X0_values)) aligned with\n",
    "    the provided orders. Rows correspond to ratios; columns to X0 values.\n",
    "    \"\"\"\n",
    "    X0_values = list(X0_values)\n",
    "    ratio_values = list(ratio_values)\n",
    "    X_final = np.zeros((len(ratio_values), len(X0_values)), dtype=float)\n",
    "\n",
    "    # Prepare tasks per ratio\n",
    "    tasks: List[Dict] = []\n",
    "    for ratio in ratio_values:\n",
    "        tasks.append({\n",
    "            \"ratio\": ratio,\n",
    "            \"X0_values\": X0_values,\n",
    "            \"I0\": I0,\n",
    "            \"beta_I\": beta_I,\n",
    "            \"b\": b,\n",
    "            \"g_I\": g_I,\n",
    "            \"T\": T,\n",
    "            \"network_type\": network_type,\n",
    "            \"n_nodes\": n_nodes,\n",
    "            \"p\": p,\n",
    "            \"m\": m,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"init_noise_I\": init_noise_I,\n",
    "            \"strategy_choice_func\": strategy_choice_func,\n",
    "            \"tau\": tau,\n",
    "        })\n",
    "\n",
    "    if max_workers is None:\n",
    "        try:\n",
    "            max_workers = os.cpu_count() or 1\n",
    "        except Exception:\n",
    "            max_workers = 1\n",
    "\n",
    "    Executor = ProcessPoolExecutor if backend == \"process\" and max_workers > 1 else ThreadPoolExecutor\n",
    "    if max_workers > 1:\n",
    "        with Executor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(_row_for_ratio_task, args) for args in tasks]\n",
    "            for i, fut in enumerate(futures):\n",
    "                row = fut.result()\n",
    "                X_final[i, :] = row\n",
    "    else:\n",
    "        for i, args in enumerate(tasks):\n",
    "            row = _row_for_ratio_task(args)\n",
    "            X_final[i, :] = row\n",
    "\n",
    "    return X_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50cb1867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final adoption: 0.3\n",
      "Final infrastructure: 0.05\n",
      "Empty DataFrame\n",
      "Columns: [X, I]\n",
      "Index: []\n",
      "['D', 'D', 'D', 'D', 'D', 'D', 'C', 'D', 'D', 'D']\n"
     ]
    }
   ],
   "source": [
    "model = EVStagHuntModel(\n",
    "    initial_ev=10,\n",
    "    a0=10.0,\n",
    "    beta_I=3.0,\n",
    "    b=10.0,\n",
    "    g_I=0.5,\n",
    "    I0=0.05,\n",
    "    n_nodes=100,\n",
    "    network_type=\"random\",\n",
    "    collect=True,\n",
    "    strategy_choice_func=\"imitate\"\n",
    ")\n",
    "\n",
    "set_initial_adopters(model, X0_frac=0.3, method=\"degree\", high=True)\n",
    "\n",
    "for _ in range(100):\n",
    "    model.step()\n",
    "\n",
    "print(\"Final adoption:\", model.get_adoption_fraction())\n",
    "print(\"Final infrastructure:\", model.infrastructure)\n",
    "\n",
    "df = model.datacollector.get_model_vars_dataframe()\n",
    "print(df)\n",
    "\n",
    "model.step()\n",
    "print([a.next_strategy for a in model.schedule.agents][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a9a658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'D', 'D', 'D', 'D', 'D', 'D', 'C', 'D', 'C']\n",
      "['D', 'D', 'D', 'D', 'D', 'C', 'D', 'D', 'D', 'D']\n"
     ]
    }
   ],
   "source": [
    "model.step()\n",
    "print([a.strategy for a in model.schedule.agents][:10])\n",
    "print([a.next_strategy for a in model.schedule.agents][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8135b953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'D', 'D', 'D', 'C', 'C', 'D', 'D', 'C', 'C', 'D', 'D', 'C', 'D', 'D', 'D', 'D', 'C', 'D', 'C']\n",
      "Adoption fraction: 0.3\n",
      "['D', 'D', 'D', 'D', 'D', 'D', 'C', 'D', 'D', 'D', 'D', 'D', 'C', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "Adoption fraction: 0.3\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    model.step()\n",
    "\n",
    "print([a.strategy for a in model.schedule.agents][:20])\n",
    "print(\"Adoption fraction:\", model.get_adoption_fraction())\n",
    "\n",
    "print([a.next_strategy for a in model.schedule.agents][:20])\n",
    "print(\"Adoption fraction:\", model.get_adoption_fraction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94f505d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adoption fraction: 0.3\n",
      "['D', 'C', 'D', 'D', 'D', 'D', 'C', 'C', 'C', 'C', 'D', 'D', 'D', 'C', 'D', 'D', 'C', 'C', 'D', 'D']\n",
      "Adoption fraction: 0.3\n",
      "['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "Adoption fraction: 0.3\n"
     ]
    }
   ],
   "source": [
    "model = EVStagHuntModel(\n",
    "    initial_ev=10,\n",
    "    a0=10.0,\n",
    "    beta_I=3.0,\n",
    "    b=10.0,\n",
    "    g_I=0.5,\n",
    "    I0=0.05,\n",
    "    n_nodes=10000,\n",
    "    network_type=\"random\",\n",
    "    collect=True,\n",
    "    strategy_choice_func=\"logit\",  # ← change here\n",
    "    tau=0.5                        # ← moderate exploration\n",
    ")\n",
    "\n",
    "set_initial_adopters(model, X0_frac=0.3, method=\"degree\", high=True)\n",
    "\n",
    "for _ in range(10000):  # fewer steps often enough\n",
    "    model.step()\n",
    "\n",
    "print(\"Adoption fraction:\", model.get_adoption_fraction())\n",
    "\n",
    "\n",
    "print([a.strategy for a in model.schedule.agents][:20])\n",
    "print(\"Adoption fraction:\", model.get_adoption_fraction())\n",
    "\n",
    "print([a.next_strategy for a in model.schedule.agents][:20])\n",
    "print(\"Adoption fraction:\", model.get_adoption_fraction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcccd22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start quick checks of model functions...\n",
      "\n",
      "1) Kleine EVStagHuntModel run (50 nodes, 30 stappen)...\n",
      "  step_count: 0\n",
      "  adoption fraction X: 0.24\n",
      "  infrastructure I: 0.05\n",
      "\n",
      "2) Single network trial (run_network_trial)...\n",
      "  final adoption from single trial: 0.26666666666666666\n",
      "\n",
      "3) Kleine ratio sweep (final_mean_adoption_vs_ratio) — kort, batch_size=4 ...\n",
      "  ratio -> mean final adoption:\n",
      "     0.8 -> 0.2500\n",
      "     1.0 -> 0.2500\n",
      "     1.2 -> 0.2500\n",
      "     1.5 -> 0.2500\n",
      "\n",
      "4) Voorbeeld _row_for_ratio_task (korte X0 lijst, batch_size=3)...\n",
      "  row (mean final adoption for each X0): [0.0, 0.05000000000000001, 0.10000000000000002]\n",
      "\n",
      "All checks finished successfully.\n",
      "\n",
      "Samenvatting resultaten:\n",
      "{'model_adoption': 0.24, 'single_trial': 0.26666666666666666, 'ratio_sweep': {0.8: 0.25, 1.0: 0.25, 1.2: 0.25, 1.5: 0.25}, 'row_example': [0.0, 0.05000000000000001, 0.10000000000000002]}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Laatste cel: alles aanroepen\n",
    "# (Plak dit precies als laatste cel; ik wijzig geen eerdere cellen)\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    import traceback\n",
    "    print(\"Start quick checks of model functions...\\n\")\n",
    "\n",
    "    try:\n",
    "        # 1) Kleine model-run (sanity)\n",
    "        print(\"1) Kleine EVStagHuntModel run (50 nodes, 30 stappen)...\")\n",
    "        model = EVStagHuntModel(\n",
    "            initial_ev=5,\n",
    "            a0=2.0,\n",
    "            beta_I=2.0,\n",
    "            b=1.0,\n",
    "            g_I=0.05,\n",
    "            I0=0.05,\n",
    "            seed=42,\n",
    "            network_type=\"random\",\n",
    "            n_nodes=50,\n",
    "            p=0.05,\n",
    "            m=2,\n",
    "            collect=True,\n",
    "            strategy_choice_func=\"imitate\",\n",
    "            tau=1.0,\n",
    "        )\n",
    "        \n",
    "        # Zorg dat er een kleine fractie adoptanten is (optioneel)\n",
    "        set_initial_adopters(model, X0_frac=0.25, method=\"random\", seed=42)\n",
    "\n",
    "        for _ in range(30):\n",
    "            model.step()\n",
    "        print(\"  step_count:\", model.step_count)\n",
    "        print(\"  adoption fraction X:\", model.get_adoption_fraction())\n",
    "        print(\"  infrastructure I:\", model.infrastructure)\n",
    "        print(\"\")\n",
    "\n",
    "        # 2) Single network trial (run_network_trial)\n",
    "        print(\"2) Single network trial (run_network_trial)...\")\n",
    "        x_star = run_network_trial(\n",
    "            X0_frac=0.25,\n",
    "            ratio=1.5,\n",
    "            I0=0.05,\n",
    "            beta_I=2.0,\n",
    "            b=5.0,\n",
    "            g_I=0.05,\n",
    "            T=200,\n",
    "            network_type=\"random\",\n",
    "            n_nodes=30,\n",
    "            p=0.05,\n",
    "            m=2,\n",
    "            seed=123,\n",
    "            tol=1e-3,\n",
    "            patience=10,\n",
    "            collect=False,\n",
    "            strategy_choice_func=\"imitate\",\n",
    "            tau=1.0,\n",
    "        )\n",
    "        print(\"  final adoption from single trial:\", x_star)\n",
    "        print(\"\")\n",
    "\n",
    "        # 3) Small ratio sweep (final_mean_adoption_vs_ratio)\n",
    "        # Let op: zet batch_size klein voor snelheid\n",
    "        print(\"3) Kleine ratio sweep (final_mean_adoption_vs_ratio) — kort, batch_size=4 ...\")\n",
    "        ratios = [0.8, 1.0, 1.2, 1.5]\n",
    "        means = final_mean_adoption_vs_ratio(\n",
    "            X0_frac=0.25,\n",
    "            ratio_values=ratios,\n",
    "            I0=0.05,\n",
    "            beta_I=0.5,\n",
    "            b=1.0,\n",
    "            g_I=0.1,\n",
    "            T=100,\n",
    "            network_type=\"random\",\n",
    "            n_nodes=100,\n",
    "            p=0.05,\n",
    "            m=2,\n",
    "            batch_size=4,          # klein om snel te draaien in notebook\n",
    "            init_noise_I=0.01,\n",
    "            strategy_choice_func=\"imitate\",\n",
    "            tau=1.0,\n",
    "        )\n",
    "        print(\"  ratio -> mean final adoption:\")\n",
    "        for r, mval in zip(ratios, means.tolist()):\n",
    "            print(f\"    {r:>4} -> {mval:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "        # 4) Row-for-ratio worker example (_row_for_ratio_task)\n",
    "        print(\"4) Voorbeeld _row_for_ratio_task (korte X0 lijst, batch_size=3)...\")\n",
    "        args = {\n",
    "            \"ratio\": 1.2,\n",
    "            \"X0_values\": [0.0, 0.05, 0.1],\n",
    "            \"I0\": 0.05,\n",
    "            \"beta_I\": 2.0,\n",
    "            \"b\": 1.0,\n",
    "            \"g_I\": 0.1,\n",
    "            \"T\": 100,\n",
    "            \"network_type\": \"random\",\n",
    "            \"n_nodes\": 100,\n",
    "            \"p\": 0.05,\n",
    "            \"m\": 2,\n",
    "            \"batch_size\": 3,        # klein om snel te doen\n",
    "            \"init_noise_I\": 0.01,\n",
    "            \"strategy_choice_func\": \"imitate\",\n",
    "            \"tau\": 1.0,\n",
    "        }\n",
    "        row = _row_for_ratio_task(args)\n",
    "        print(\"  row (mean final adoption for each X0):\", row.tolist())\n",
    "        print(\"\")\n",
    "\n",
    "        result_summary = {\n",
    "            \"model_adoption\": model.get_adoption_fraction(),\n",
    "            \"single_trial\": x_star,\n",
    "            \"ratio_sweep\": {float(r): float(v) for r, v in zip(ratios, means.tolist())},\n",
    "            \"row_example\": [float(v) for v in row.tolist()],\n",
    "        }\n",
    "\n",
    "        print(\"All checks finished successfully.\")\n",
    "        return result_summary\n",
    "\n",
    "    except Exception as exc:\n",
    "        print(\"Er trad een fout op tijdens het uitvoeren van de checks:\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# In Jupyter is het gebruikelijk om main() direct te draaien:\n",
    "results = main()\n",
    "print(\"\\nSamenvatting resultaten:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d9668840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial adoption: 0.25\n",
      "\n",
      "--- BEFORE STEP 0 ---\n",
      "id=0, strategy=D, payoff=None\n",
      "id=1, strategy=C, payoff=None\n",
      "id=2, strategy=D, payoff=None\n",
      "id=3, strategy=D, payoff=None\n",
      "id=4, strategy=D, payoff=None\n",
      "id=5, strategy=D, payoff=None\n",
      "id=6, strategy=D, payoff=None\n",
      "id=7, strategy=D, payoff=None\n",
      "id=8, strategy=C, payoff=None\n",
      "id=9, strategy=D, payoff=None\n",
      "id=10, strategy=D, payoff=None\n",
      "id=11, strategy=C, payoff=None\n",
      "id=12, strategy=D, payoff=None\n",
      "id=13, strategy=C, payoff=None\n",
      "id=14, strategy=D, payoff=None\n",
      "id=15, strategy=D, payoff=None\n",
      "id=16, strategy=D, payoff=None\n",
      "id=17, strategy=D, payoff=None\n",
      "id=18, strategy=D, payoff=None\n",
      "id=19, strategy=C, payoff=None\n",
      "\n",
      "--- AFTER STEP 0 ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EVAgent' object has no attribute 'last_payoff'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- AFTER STEP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ag \u001b[38;5;129;01min\u001b[39;00m model.agents:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mid=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag.unique_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, strategy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mag.strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, payoff=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mag\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlast_payoff\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAdoption fraction:\u001b[39m\u001b[33m\"\u001b[39m, model.get_adoption_fraction())\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'EVAgent' object has no attribute 'last_payoff'"
     ]
    }
   ],
   "source": [
    "model = EVStagHuntModel(\n",
    "    initial_ev=5,\n",
    "    a0=2.0,\n",
    "    beta_I=2.0,\n",
    "    b=1.0,\n",
    "    g_I=0.05,\n",
    "    I0=0.05,\n",
    "    seed=42,\n",
    "    network_type=\"random\",\n",
    "    n_nodes=20,\n",
    "    p=0.05,\n",
    "    m=2,\n",
    "    collect=False,\n",
    "    strategy_choice_func=\"imitate\",\n",
    "    tau=1.0,\n",
    ")\n",
    "\n",
    "set_initial_adopters(model, X0_frac=0.25, method=\"random\", seed=42)\n",
    "\n",
    "print(\"Initial adoption:\", model.get_adoption_fraction())\n",
    "print()\n",
    "\n",
    "for t in range(3):\n",
    "    print(f\"--- BEFORE STEP {t} ---\")\n",
    "    for ag in model.agents:\n",
    "        print(f\"id={ag.unique_id}, strategy={ag.strategy}, payoff={getattr(ag, 'last_payoff', None)}\")\n",
    "\n",
    "    model.step()\n",
    "\n",
    "    print(f\"\\n--- AFTER STEP {t} ---\")\n",
    "    for ag in model.agents:\n",
    "        print(f\"id={ag.unique_id}, strategy={ag.strategy}, payoff={ag.last_payoff}\")\n",
    "\n",
    "    print(\"Adoption fraction:\", model.get_adoption_fraction())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d07ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
