{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ddc71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mesa import Agent, Model\n",
    "from mesa.time import SimultaneousActivation\n",
    "from mesa.space import NetworkGrid\n",
    "from mesa.datacollection import DataCollector\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Iterable, List, Dict\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9518b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Strategy selection helpers\n",
    "#\n",
    "# We provide two different ways agents can choose their strategy:\n",
    "# - `choose_strategy_imitate`: choose strategy of the highest-payoff neighbour (including self).\n",
    "# - `choose_strategy_logit`: choose strategy using logit / softmax choice.\n",
    "#\n",
    "####################################\n",
    "def choose_strategy_imitate(agent, neighbors):\n",
    "    \"\"\"Choose strategy of the highest-payoff neighbour (including self).\"\"\"\n",
    "    candidates = neighbors + [agent]\n",
    "    best = max(candidates, key=lambda a: a.payoff)\n",
    "    return best.strategy\n",
    "\n",
    "def choose_strategy_logit(agent, neighbors, a_I, b, tau):\n",
    "    \"\"\"Choose strategy using logit / softmax choice.\n",
    "\n",
    "    Parameters\n",
    "    - agent: the agent choosing a strategy\n",
    "    - neighbors: list of neighbour agents\n",
    "    - a_I: effective coordination payoff given current infrastructure\n",
    "    - b: defection payoff\n",
    "    - tau: temperature parameter for softmax\n",
    "    \"\"\"\n",
    "    # compute expected payoffs for C and D\n",
    "    pi_C = 0.0\n",
    "    pi_D = 0.0\n",
    "    for other in neighbors:\n",
    "        s_j = other.strategy\n",
    "        if s_j == \"C\":\n",
    "            pi_C += a_I\n",
    "            pi_D += b\n",
    "        else:\n",
    "            pi_C += 0.0\n",
    "            pi_D += b\n",
    "\n",
    "    # softmax choice\n",
    "    denom = np.exp(pi_C / tau) + np.exp(pi_D / tau)\n",
    "    P_C = np.exp(pi_C / tau) / denom if denom > 0 else 0.5\n",
    "    return \"C\" if random.random() < P_C else \"D\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be1df003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVAgent(Agent):\n",
    "    \"\"\"Agent for EV Stag Hunt model (SimultaneousActivation-friendly).\n",
    "\n",
    "    - step(): compute payoff AND decide next_strategy (do NOT commit)\n",
    "    - advance(): commit next_strategy -> strategy\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, model, init_strategy=\"D\"):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.strategy = init_strategy\n",
    "        self.payoff = 0.0\n",
    "        self.next_strategy = init_strategy\n",
    "\n",
    "    def _get_neighbor_agents(self):\n",
    "        \"\"\"Return list of agent objects that are neighbors on the network grid.\"\"\"\n",
    "        neighbors = []\n",
    "        # self.pos is the node id in NetworkGrid\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbors.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "        return neighbors\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Compute payoff from neighbors and choose next_strategy (but do NOT commit).\"\"\"\n",
    "        I = self.model.infrastructure\n",
    "        a0 = self.model.a0\n",
    "        beta_I = self.model.beta_I\n",
    "        b = self.model.b\n",
    "        a_I = a0 + beta_I * I\n",
    "\n",
    "        neighbor_agents = self._get_neighbor_agents()\n",
    "\n",
    "        # If no neighbors: no interactions, keep payoff 0 and keep current strategy\n",
    "        if not neighbor_agents:\n",
    "            self.payoff = 0.0\n",
    "            self.next_strategy = self.strategy\n",
    "            return\n",
    "\n",
    "        # Compute payoff (sum over pairwise interactions)\n",
    "        payoff = 0.0\n",
    "        for other in neighbor_agents:\n",
    "            s_i = self.strategy\n",
    "            s_j = other.strategy\n",
    "            if s_i == \"C\" and s_j == \"C\":\n",
    "                payoff += a_I\n",
    "            elif s_i == \"C\" and s_j == \"D\":\n",
    "                payoff += 0.0\n",
    "            elif s_i == \"D\" and s_j == \"C\":\n",
    "                payoff += b\n",
    "            else:  # D vs D\n",
    "                payoff += b\n",
    "        self.payoff = payoff\n",
    "\n",
    "        # Decide next strategy (do NOT assign to self.strategy here)\n",
    "        func = getattr(self.model, \"strategy_choice_func\", \"imitate\")\n",
    "        if func == \"imitate\":\n",
    "            # expected signature: choose_strategy_imitate(agent, neighbor_agents) -> \"C\" or \"D\"\n",
    "            self.next_strategy = choose_strategy_imitate(self, neighbor_agents)\n",
    "        elif func == \"logit\":\n",
    "            tau = getattr(self.model, \"tau\", 1.0)\n",
    "            # expected signature: choose_strategy_logit(agent, neighbor_agents, a_I, b, tau)\n",
    "            self.next_strategy = choose_strategy_logit(self, neighbor_agents, a_I, b, tau)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy choice function: {func}\")\n",
    "\n",
    "    def advance(self):\n",
    "        \"\"\"Commit the previously chosen next_strategy (synchronous update).\"\"\"\n",
    "        # simply commit choice (no recomputation)\n",
    "        self.strategy = self.next_strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "254a6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ####################################\n",
    "    # Model class\n",
    "    #\n",
    "    # The EVStagHuntModel class implements the Mesa model for EV Stag Hunt on a network.\n",
    "    #\n",
    "    # Parameters\n",
    "    # - initial_ev: number of initial EV nodes\n",
    "    # - a0: base payoff for EV adoption\n",
    "    # - beta_I: payoff enhancement factor for EV adoption\n",
    "    # - b: payoff for ICE defection\n",
    "    # - g_I: infrastructure growth rate\n",
    "    # - I0: initial infrastructure level\n",
    "    # - seed: random seed for reproducibility\n",
    "    # - network_type: type of network to generate (\"random\" or \"BA\")\n",
    "    # - n_nodes: number of nodes in the network\n",
    "    # - p: probability of edge creation in random network\n",
    "    # - m: number of edges to attach to new node in BA network\n",
    "    # - collect: whether to collect agent and model-level data\n",
    "    # - strategy_choice_func: strategy selection function (\"imitate\" or \"logit\")\n",
    "    # - tau: temperature parameter for softmax choice (only used with \"logit\")\n",
    "    ####################################\n",
    "#\n",
    "class EVStagHuntModel(Model):\n",
    "    \"\"\"Mesa model for EV Stag Hunt on a network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_ev=10,\n",
    "        a0=2.0,\n",
    "        beta_I=3.0,\n",
    "        b=1.0,\n",
    "        g_I=0.1,\n",
    "        I0=0.05,\n",
    "        seed=None,\n",
    "        network_type=\"random\",\n",
    "        n_nodes=100,\n",
    "        p=0.05,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        strategy_choice_func: str = \"imitate\",\n",
    "        tau: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(seed=seed)\n",
    "\n",
    "        # Build graph\n",
    "        if network_type == \"BA\":\n",
    "            G = nx.barabasi_albert_graph(n_nodes, m, seed=seed)\n",
    "        else:\n",
    "            G = nx.erdos_renyi_graph(n_nodes, p, seed=seed)\n",
    "        self.G = G\n",
    "        self.grid = NetworkGrid(G)\n",
    "        self.schedule = SimultaneousActivation(self)\n",
    "\n",
    "        # parameters\n",
    "        self.a0 = a0\n",
    "        self.beta_I = beta_I\n",
    "        self.b = b\n",
    "        self.g_I = g_I\n",
    "        self.infrastructure = I0\n",
    "        self.step_count = 0\n",
    "        self.strategy_choice_func = strategy_choice_func\n",
    "        self.tau = tau\n",
    "\n",
    "        # initialize node attribute for agent reference\n",
    "        for n in self.G.nodes:\n",
    "            self.G.nodes[n][\"agent\"] = []\n",
    "\n",
    "        # choose initial EV nodes\n",
    "        total_nodes = self.G.number_of_nodes()\n",
    "        k_ev = max(0, min(initial_ev, total_nodes))\n",
    "        ev_nodes = set(self.random.sample(list(self.G.nodes), k_ev))\n",
    "\n",
    "        # create one agent per node\n",
    "        uid = 0\n",
    "        for node in self.G.nodes:\n",
    "            init_strategy = \"C\" if node in ev_nodes else \"D\"\n",
    "            agent = EVAgent(uid, self, init_strategy)\n",
    "            uid += 1\n",
    "            self.schedule.add(agent)\n",
    "            self.grid.place_agent(agent, node)\n",
    "\n",
    "        self.datacollector = None\n",
    "        if collect:\n",
    "            self.datacollector = DataCollector(\n",
    "                model_reporters={\n",
    "                    \"X\": self.get_adoption_fraction,\n",
    "                    \"I\": lambda m: m.infrastructure,\n",
    "                },\n",
    "                agent_reporters={\"strategy\": \"strategy\", \"payoff\": \"payoff\"},\n",
    "            )\n",
    "\n",
    "    def get_adoption_fraction(self):\n",
    "        agents = self.schedule.agents\n",
    "        if not agents:\n",
    "            return 0.0\n",
    "        return sum(1 for a in agents if a.strategy == \"C\") / len(agents)\n",
    "    \n",
    "    # ####################\n",
    "    # Model step function\n",
    "    #\n",
    "    # The step function advances the model by one time step.\n",
    "    # It first advances all agents, then computes the adoption fraction and infrastructure level.\n",
    "    # The infrastructure level is updated based on the adoption fraction and the infrastructure growth rate.\n",
    "    # The updated infrastructure level is clipped to the interval [0, 1].\n",
    "    # Finally, if data collection is enabled, the model and agent data are collected.\n",
    "    #######################\n",
    "    \n",
    "    def step(self): \n",
    "        self.schedule.step() # advance all agents\n",
    "\n",
    "        X = self.get_adoption_fraction() # compute adoption fraction after all agents have advanced\n",
    "        I = self.infrastructure # infrastructure level before this step\n",
    "        dI = self.g_I * (X - I) # infrastructure growth rate, impacted by adoption fraction\n",
    "        self.infrastructure = float(min(1.0, max(0.0, I + dI))) # clip infrastructure level to [0, 1]\n",
    "    \n",
    "        if self.datacollector is not None:\n",
    "            self.datacollector.collect(self) # collect data at the end of each step\n",
    "            \n",
    "        self.step_count += 1 # increment step count after data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25d403b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################\n",
    "#\n",
    "# Set initial adopters\n",
    "# \n",
    "# Parameters\n",
    "# - model: the EVStagHuntModel instance\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - method: method to choose initial adopters (\"random\" or \"degree\")\n",
    "# - seed: random seed for reproducibility\n",
    "# - high: whether to choose high or low degree nodes for \"degree\" method\n",
    "###########################\n",
    "def set_initial_adopters(model, X0_frac, method=\"random\", seed=None, high=True):\n",
    "    \"\"\"Set a fraction of agents to EV adopters using different heuristics.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    agents = model.schedule.agents\n",
    "    n = len(agents)\n",
    "    k = int(round(X0_frac * n))\n",
    "\n",
    "    for a in agents:\n",
    "        a.strategy = \"D\"\n",
    "\n",
    "    if k <= 0:\n",
    "        return\n",
    "\n",
    "    if method == \"random\":\n",
    "        idx = rng.choice(n, size=k, replace=False)\n",
    "        for i in idx:\n",
    "            agents[i].strategy = \"C\"\n",
    "        return\n",
    "\n",
    "    if method == \"degree\":\n",
    "        deg = dict(model.G.degree())\n",
    "        ordered_nodes = sorted(deg.keys(), key=lambda u: deg[u], reverse=high)\n",
    "        chosen = set(ordered_nodes[:k])\n",
    "        for a in agents:\n",
    "            if a.unique_id in chosen:\n",
    "                a.strategy = \"C\"\n",
    "        return\n",
    "\n",
    "    raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae041a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Ratio sweep helpers (computation-only)\n",
    "# -----------------------------\n",
    "#########################\n",
    "#\n",
    "# Run a single network trial\n",
    "# \n",
    "# Parameters\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - ratio: payoff ratio between EV and DC agents (a0 = ratio*b - beta_I*I0)\n",
    "# - I0: initial infrastructure level\n",
    "# - beta_I: cost of EV adoption relative to DC (beta_I*I0)\n",
    "# - b: payoff of EV (b)\n",
    "# - g_I: infrastructure growth rate (g_I)\n",
    "# - T: number of time steps to run\n",
    "# - network_type: type of network to generate (\"random\" or \"BA\")\n",
    "# - n_nodes: number of nodes in the network\n",
    "# - p: probability of edge creation in random network\n",
    "# - m: number of edges to attach from a new node to existing nodes in BA network\n",
    "# - seed: random seed for reproducibility\n",
    "# - tol: tolerance for convergence check (default: 1e-3)\n",
    "# - patience: number of steps to wait for convergence (default: 30)\n",
    "\n",
    "def run_network_trial(\n",
    "    X0_frac: float,\n",
    "    ratio: float,\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    seed: int | None = None,\n",
    "    tol: float = 1e-3,\n",
    "    patience: int = 30,\n",
    "    collect: bool = False,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"Run a single realisation and return final adoption fraction.\n",
    "\n",
    "    Preserves the intended initial payoff ratio via a0 = ratio*b - beta_I*I0.\n",
    "    Includes basic stability-based early stopping.\n",
    "    \"\"\"\n",
    "    initial_ev = int(round(X0_frac * n_nodes))\n",
    "    a0 = ratio * b - beta_I * I0\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        initial_ev=initial_ev,\n",
    "        a0=a0,\n",
    "        beta_I=beta_I,\n",
    "        b=b,\n",
    "        g_I=g_I,\n",
    "        I0=I0,\n",
    "        seed=seed,\n",
    "        network_type=network_type,\n",
    "        n_nodes=n_nodes,\n",
    "        p=p,\n",
    "        m=m,\n",
    "        collect=collect,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    stable_steps = 0\n",
    "    prev_X = None\n",
    "    prev_I = None\n",
    "    for _ in range(T):\n",
    "        model.step()\n",
    "        X = model.get_adoption_fraction()\n",
    "        I = model.infrastructure\n",
    "        if prev_X is not None and prev_I is not None:\n",
    "            if abs(X - prev_X) < tol and abs(I - prev_I) < tol:\n",
    "                stable_steps += 1\n",
    "            else:\n",
    "                stable_steps = 0\n",
    "        prev_X, prev_I = X, I\n",
    "        if X in (0.0, 1.0) and stable_steps >= 10:\n",
    "            break\n",
    "        if stable_steps >= patience:\n",
    "            break\n",
    "\n",
    "    return model.get_adoption_fraction()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4db1553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################\n",
    "#\n",
    "# Compute final mean adoption fraction vs ratio\n",
    "# \n",
    "##########################\n",
    "def final_mean_adoption_vs_ratio(\n",
    "    X0_frac: float,\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute mean final adoption across a sweep of ratio values.\n",
    "\n",
    "    For each ratio, average over `batch_size` trials with jittered `I0` and seeds.\n",
    "    Returns a numpy array of means aligned with `ratio_values` order.\n",
    "    \"\"\"\n",
    "    ratios = list(ratio_values)\n",
    "    means: List[float] = []\n",
    "    for ratio in ratios:\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        means.append(float(np.mean(finals)))\n",
    "    return np.asarray(means, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a5974b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################\n",
    "#\n",
    "# Compute heatmap row for a fixed ratio\n",
    "# \n",
    "##########################\n",
    "def _row_for_ratio_task(args: Dict) -> np.ndarray:\n",
    "    \"\"\"Top-level worker to compute one heatmap row for a fixed ratio.\n",
    "\n",
    "    Returns an array of mean final adoption across provided X0_values.\n",
    "    \"\"\"\n",
    "    ratio = args[\"ratio\"]\n",
    "    X0_values = args[\"X0_values\"]\n",
    "    I0 = args[\"I0\"]\n",
    "    beta_I = args[\"beta_I\"]\n",
    "    b = args[\"b\"]\n",
    "    g_I = args[\"g_I\"]\n",
    "    T = args[\"T\"]\n",
    "    network_type = args[\"network_type\"]\n",
    "    n_nodes = args[\"n_nodes\"]\n",
    "    p = args[\"p\"]\n",
    "    m = args[\"m\"]\n",
    "    batch_size = args[\"batch_size\"]\n",
    "    init_noise_I = args[\"init_noise_I\"]\n",
    "    strategy_choice_func = args[\"strategy_choice_func\"]\n",
    "    tau = args[\"tau\"]\n",
    "\n",
    "    row = np.empty(len(X0_values), dtype=float)\n",
    "    for j, X0 in enumerate(X0_values):\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac=X0,\n",
    "                ratio=ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        row[j] = float(np.mean(finals))\n",
    "    return row\n",
    "\n",
    "    \n",
    "#########################\n",
    "#\n",
    "# Compute heatmap matrix for phase sweep\n",
    "# \n",
    "##########################\n",
    "def phase_sweep_X0_vs_ratio(\n",
    "    X0_values: Iterable[float],\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 250,\n",
    "    network_type: str = \"BA\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute a heatmap matrix of mean final adoption X* over (X0, ratio).\n",
    "\n",
    "    Returns an array of shape (len(ratio_values), len(X0_values)) aligned with\n",
    "    the provided orders. Rows correspond to ratios; columns to X0 values.\n",
    "    \"\"\"\n",
    "    X0_values = list(X0_values)\n",
    "    ratio_values = list(ratio_values)\n",
    "    X_final = np.zeros((len(ratio_values), len(X0_values)), dtype=float)\n",
    "\n",
    "    # Prepare tasks per ratio\n",
    "    tasks: List[Dict] = []\n",
    "    for ratio in ratio_values:\n",
    "        tasks.append({\n",
    "            \"ratio\": ratio,\n",
    "            \"X0_values\": X0_values,\n",
    "            \"I0\": I0,\n",
    "            \"beta_I\": beta_I,\n",
    "            \"b\": b,\n",
    "            \"g_I\": g_I,\n",
    "            \"T\": T,\n",
    "            \"network_type\": network_type,\n",
    "            \"n_nodes\": n_nodes,\n",
    "            \"p\": p,\n",
    "            \"m\": m,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"init_noise_I\": init_noise_I,\n",
    "            \"strategy_choice_func\": strategy_choice_func,\n",
    "            \"tau\": tau,\n",
    "        })\n",
    "\n",
    "    if max_workers is None:\n",
    "        try:\n",
    "            max_workers = os.cpu_count() or 1\n",
    "        except Exception:\n",
    "            max_workers = 1\n",
    "\n",
    "    Executor = ProcessPoolExecutor if backend == \"process\" and max_workers > 1 else ThreadPoolExecutor\n",
    "    if max_workers > 1:\n",
    "        with Executor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(_row_for_ratio_task, args) for args in tasks]\n",
    "            for i, fut in enumerate(futures):\n",
    "                row = fut.result()\n",
    "                X_final[i, :] = row\n",
    "    else:\n",
    "        for i, args in enumerate(tasks):\n",
    "            row = _row_for_ratio_task(args)\n",
    "            X_final[i, :] = row\n",
    "\n",
    "    return X_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "50cb1867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final adoption: 0.21\n",
      "Final infrastructure: 0.21000000000000002\n",
      "       X         I\n",
      "0   0.38  0.215000\n",
      "1   0.39  0.302500\n",
      "2   0.36  0.331250\n",
      "3   0.24  0.285625\n",
      "4   0.22  0.252812\n",
      "..   ...       ...\n",
      "95  0.21  0.210000\n",
      "96  0.21  0.210000\n",
      "97  0.21  0.210000\n",
      "98  0.21  0.210000\n",
      "99  0.21  0.210000\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "['D', 'D', 'C', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n"
     ]
    }
   ],
   "source": [
    "model = EVStagHuntModel(\n",
    "    initial_ev=10,\n",
    "    a0=10.0,\n",
    "    beta_I=3.0,\n",
    "    b=10.0,\n",
    "    g_I=0.5,\n",
    "    I0=0.05,\n",
    "    n_nodes=100,\n",
    "    network_type=\"random\",\n",
    "    collect=True,\n",
    "    strategy_choice_func=\"imitate\"\n",
    ")\n",
    "\n",
    "set_initial_adopters(model, X0_frac=0.3, method=\"degree\", high=True)\n",
    "\n",
    "for _ in range(100):\n",
    "    model.step()\n",
    "\n",
    "print(\"Final adoption:\", model.get_adoption_fraction())\n",
    "print(\"Final infrastructure:\", model.infrastructure)\n",
    "\n",
    "df = model.datacollector.get_model_vars_dataframe()\n",
    "print(df)\n",
    "\n",
    "model.step()\n",
    "print([a.next_strategy for a in model.schedule.agents][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a9a658f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'D', 'C', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "['D', 'D', 'C', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n"
     ]
    }
   ],
   "source": [
    "model.step()\n",
    "print([a.strategy for a in model.schedule.agents][:10])\n",
    "print([a.next_strategy for a in model.schedule.agents][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8135b953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D', 'D', 'C', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'C', 'D', 'D', 'C', 'D', 'C', 'D', 'D', 'D']\n",
      "Adoption fraction: 0.21\n",
      "['D', 'D', 'C', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'C', 'D', 'D', 'C', 'D', 'C', 'D', 'D', 'D']\n",
      "Adoption fraction: 0.21\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    model.step()\n",
    "\n",
    "print([a.strategy for a in model.schedule.agents][:20])\n",
    "print(\"Adoption fraction:\", model.get_adoption_fraction())\n",
    "\n",
    "print([a.next_strategy for a in model.schedule.agents][:20])\n",
    "print(\"Adoption fraction:\", model.get_adoption_fraction())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94f505d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m set_initial_adopters(model, X0_frac=\u001b[32m0.3\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mdegree\u001b[39m\u001b[33m\"\u001b[39m, high=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10000\u001b[39m):  \u001b[38;5;66;03m# fewer steps often enough\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAdoption fraction:\u001b[39m\u001b[33m\"\u001b[39m, model.get_adoption_fraction())\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m([a.strategy \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m model.schedule.agents][:\u001b[32m20\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mEVStagHuntModel.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m): \n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# advance all agents\u001b[39;00m\n\u001b[32m    111\u001b[39m     X = \u001b[38;5;28mself\u001b[39m.get_adoption_fraction() \u001b[38;5;66;03m# compute adoption fraction after all agents have advanced\u001b[39;00m\n\u001b[32m    112\u001b[39m     I = \u001b[38;5;28mself\u001b[39m.infrastructure \u001b[38;5;66;03m# infrastructure level before this step\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Model-based-decision-making/venv/lib/python3.13/site-packages/mesa/time.py:120\u001b[39m, in \u001b[36mBaseScheduler._wrapped_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    119\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper for the step method to include time and step updating.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_original_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m.model._advance_time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Model-based-decision-making/venv/lib/python3.13/site-packages/mesa/time.py:200\u001b[39m, in \u001b[36mSimultaneousActivation.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    199\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Step all agents, then advance them.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_each\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# do_each recomputes the agent_keys from scratch whenever it is called.\u001b[39;00m\n\u001b[32m    202\u001b[39m     \u001b[38;5;66;03m# It can handle the case when some agents might have been removed in\u001b[39;00m\n\u001b[32m    203\u001b[39m     \u001b[38;5;66;03m# the previous loop.\u001b[39;00m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28mself\u001b[39m.do_each(\u001b[33m\"\u001b[39m\u001b[33madvance\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Model-based-decision-making/venv/lib/python3.13/site-packages/mesa/time.py:152\u001b[39m, in \u001b[36mBaseScheduler.do_each\u001b[39m\u001b[34m(self, method, shuffle)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m._agents.shuffle(inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_agents\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Model-based-decision-making/venv/lib/python3.13/site-packages/mesa/agent.py:269\u001b[39m, in \u001b[36mAgentSet.do\u001b[39m\u001b[34m(self, method, *args, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m agentref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agents.keyrefs():\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (agent := agentref()) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m             \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m agentref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agents.keyrefs():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mEVAgent.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     59\u001b[39m     tau = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mtau\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# expected signature: choose_strategy_logit(agent, neighbor_agents, a_I, b, tau)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28mself\u001b[39m.next_strategy = \u001b[43mchoose_strategy_logit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbor_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_I\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown strategy choice function: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mchoose_strategy_logit\u001b[39m\u001b[34m(agent, neighbors, a_I, b, tau)\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     34\u001b[39m         pi_C += \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         pi_D += b\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# softmax choice\u001b[39;00m\n\u001b[32m     38\u001b[39m denom = np.exp(pi_C / tau) + np.exp(pi_D / tau)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = EVStagHuntModel(\n",
    "    initial_ev=10,\n",
    "    a0=10.0,\n",
    "    beta_I=3.0,\n",
    "    b=10.0,\n",
    "    g_I=0.5,\n",
    "    I0=0.05,\n",
    "    n_nodes=50,\n",
    "    network_type=\"random\",\n",
    "    collect=True,\n",
    "    strategy_choice_func=\"logit\",  # ← change here\n",
    "    tau=0.5                        # ← moderate exploration\n",
    ")\n",
    "\n",
    "set_initial_adopters(model, X0_frac=0.3, method=\"degree\", high=True)\n",
    "\n",
    "for _ in range(10000):  # fewer steps often enough\n",
    "    model.step()\n",
    "\n",
    "print(\"Adoption fraction:\", model.get_adoption_fraction())\n",
    "\n",
    "\n",
    "print([a.strategy for a in model.schedule.agents][:20])\n",
    "print(\"Adoption fraction:\", model.get_adoption_fraction())\n",
    "\n",
    "print([a.next_strategy for a in model.schedule.agents][:20])\n",
    "print(\"Adoption fraction:\", model.get_adoption_fraction())\n",
    "\n",
    "for _ in range(30):\n",
    "    model.step()\n",
    "print(\"  step_count:\", model.step_count)\n",
    "print(\"  adoption fraction X:\", model.get_adoption_fraction())\n",
    "print(\"  infrastructure I:\", model.infrastructure)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dcccd22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start quick checks of model functions...\n",
      "\n",
      "1) Kleine EVStagHuntModel run (50 nodes, 30 stappen)...\n",
      "  step_count: 30\n",
      "  adoption fraction X: 0.12\n",
      "  infrastructure I: 0.09708522195408047\n",
      "\n",
      "2) Single network trial (run_network_trial)...\n",
      "  final adoption from single trial: 0.03333333333333333\n",
      "\n",
      "3) Kleine ratio sweep (final_mean_adoption_vs_ratio) — kort, batch_size=4 ...\n",
      "  ratio -> mean final adoption:\n",
      "     0.8 -> 0.0050\n",
      "     1.0 -> 0.0050\n",
      "     1.2 -> 0.0025\n",
      "     1.5 -> 0.0050\n",
      "\n",
      "4) Voorbeeld _row_for_ratio_task (korte X0 lijst, batch_size=3)...\n",
      "  row (mean final adoption for each X0): [0.0, 0.0, 0.0]\n",
      "\n",
      "All checks finished successfully.\n",
      "\n",
      "Samenvatting resultaten:\n",
      "{'model_adoption': 0.12, 'single_trial': 0.03333333333333333, 'ratio_sweep': {0.8: 0.005, 1.0: 0.005, 1.2: 0.0025, 1.5: 0.005}, 'row_example': [0.0, 0.0, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Laatste cel: alles aanroepen\n",
    "# (Plak dit precies als laatste cel; ik wijzig geen eerdere cellen)\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    import traceback\n",
    "    print(\"Start quick checks of model functions...\\n\")\n",
    "\n",
    "    try:\n",
    "        # 1) Kleine model-run (sanity)\n",
    "        print(\"1) Kleine EVStagHuntModel run (50 nodes, 30 stappen)...\")\n",
    "        model = EVStagHuntModel(\n",
    "            initial_ev=5,\n",
    "            a0=2.0,\n",
    "            beta_I=2.0,\n",
    "            b=1.0,\n",
    "            g_I=0.05,\n",
    "            I0=0.05,\n",
    "            seed=42,\n",
    "            network_type=\"random\",\n",
    "            n_nodes=50,\n",
    "            p=0.05,\n",
    "            m=2,\n",
    "            collect=True,\n",
    "            strategy_choice_func=\"imitate\",\n",
    "            tau=1.0,\n",
    "        )\n",
    "        \n",
    "        # Zorg dat er een kleine fractie adoptanten is (optioneel)\n",
    "        set_initial_adopters(model, X0_frac=0.25, method=\"random\", seed=42)\n",
    "\n",
    "        for _ in range(30):\n",
    "            model.step()\n",
    "        print(\"  step_count:\", model.step_count)\n",
    "        print(\"  adoption fraction X:\", model.get_adoption_fraction())\n",
    "        print(\"  infrastructure I:\", model.infrastructure)\n",
    "        print(\"\")\n",
    "\n",
    "        # 2) Single network trial (run_network_trial)\n",
    "        print(\"2) Single network trial (run_network_trial)...\")\n",
    "        x_star = run_network_trial(\n",
    "            X0_frac=0.25,\n",
    "            ratio=1.5,\n",
    "            I0=0.05,\n",
    "            beta_I=2.0,\n",
    "            b=5.0,\n",
    "            g_I=0.05,\n",
    "            T=200,\n",
    "            network_type=\"random\",\n",
    "            n_nodes=30,\n",
    "            p=0.05,\n",
    "            m=2,\n",
    "            seed=123,\n",
    "            tol=1e-3,\n",
    "            patience=10,\n",
    "            collect=False,\n",
    "            strategy_choice_func=\"imitate\",\n",
    "            tau=1.0,\n",
    "        )\n",
    "        print(\"  final adoption from single trial:\", x_star)\n",
    "        print(\"\")\n",
    "\n",
    "        # 3) Small ratio sweep (final_mean_adoption_vs_ratio)\n",
    "        # Let op: zet batch_size klein voor snelheid\n",
    "        print(\"3) Kleine ratio sweep (final_mean_adoption_vs_ratio) — kort, batch_size=4 ...\")\n",
    "        ratios = [0.8, 1.0, 1.2, 1.5]\n",
    "        means = final_mean_adoption_vs_ratio(\n",
    "            X0_frac=0.25,\n",
    "            ratio_values=ratios,\n",
    "            I0=0.05,\n",
    "            beta_I=0.5,\n",
    "            b=1.0,\n",
    "            g_I=0.1,\n",
    "            T=100,\n",
    "            network_type=\"random\",\n",
    "            n_nodes=100,\n",
    "            p=0.05,\n",
    "            m=2,\n",
    "            batch_size=4,          # klein om snel te draaien in notebook\n",
    "            init_noise_I=0.01,\n",
    "            strategy_choice_func=\"imitate\",\n",
    "            tau=1.0,\n",
    "        )\n",
    "        print(\"  ratio -> mean final adoption:\")\n",
    "        for r, mval in zip(ratios, means.tolist()):\n",
    "            print(f\"    {r:>4} -> {mval:.4f}\")\n",
    "        print(\"\")\n",
    "\n",
    "        # 4) Row-for-ratio worker example (_row_for_ratio_task)\n",
    "        print(\"4) Voorbeeld _row_for_ratio_task (korte X0 lijst, batch_size=3)...\")\n",
    "        args = {\n",
    "            \"ratio\": 1.2,\n",
    "            \"X0_values\": [0.0, 0.05, 0.1],\n",
    "            \"I0\": 0.05,\n",
    "            \"beta_I\": 2.0,\n",
    "            \"b\": 1.0,\n",
    "            \"g_I\": 0.1,\n",
    "            \"T\": 100,\n",
    "            \"network_type\": \"random\",\n",
    "            \"n_nodes\": 100,\n",
    "            \"p\": 0.05,\n",
    "            \"m\": 2,\n",
    "            \"batch_size\": 3,        # klein om snel te doen\n",
    "            \"init_noise_I\": 0.01,\n",
    "            \"strategy_choice_func\": \"imitate\",\n",
    "            \"tau\": 1.0,\n",
    "        }\n",
    "        row = _row_for_ratio_task(args)\n",
    "        print(\"  row (mean final adoption for each X0):\", row.tolist())\n",
    "        print(\"\")\n",
    "\n",
    "        result_summary = {\n",
    "            \"model_adoption\": model.get_adoption_fraction(),\n",
    "            \"single_trial\": x_star,\n",
    "            \"ratio_sweep\": {float(r): float(v) for r, v in zip(ratios, means.tolist())},\n",
    "            \"row_example\": [float(v) for v in row.tolist()],\n",
    "        }\n",
    "\n",
    "        print(\"All checks finished successfully.\")\n",
    "        return result_summary\n",
    "\n",
    "    except Exception as exc:\n",
    "        print(\"Er trad een fout op tijdens het uitvoeren van de checks:\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# In Jupyter is het gebruikelijk om main() direct te draaien:\n",
    "results = main()\n",
    "print(\"\\nSamenvatting resultaten:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b18f1",
   "metadata": {},
   "source": [
    "## code van ev_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23173e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting utilities for EV Stag Hunt experiments.\n",
    "\n",
    "All functions accept pandas DataFrames produced by ev_experiments\n",
    "and save figures to disk, returning the output path.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _default_plot_path(filename: str) -> str:\n",
    "    plots_dir = os.path.join(os.getcwd(), \"plots\")\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    return os.path.join(plots_dir, filename)\n",
    "\n",
    "\n",
    "def plot_fanchart(traces_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot fan charts (quantile bands) for baseline vs subsidy using traces DF.\n",
    "\n",
    "    traces_df columns: ['group', 'trial', 'time', 'X'] where group in {'baseline','subsidy'}.\n",
    "    \"\"\"\n",
    "    if traces_df.empty:\n",
    "        raise ValueError(\"traces_df is empty\")\n",
    "\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(11, 8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "\n",
    "        # Compute quantiles by time across trials\n",
    "        q = gdf.groupby(\"time\")[\"X\"].quantile([0.10, 0.25, 0.75, 0.90]).unstack(level=1)\n",
    "        mean = gdf.groupby(\"time\")[\"X\"].mean()\n",
    "        t = mean.index.to_numpy()\n",
    "\n",
    "        ax = axes[0, j]\n",
    "        ax.fill_between(t, q[0.10], q[0.90], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.15, label=\"10–90%\")\n",
    "        ax.fill_between(t, q[0.25], q[0.75], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.30, label=\"25–75%\")\n",
    "\n",
    "        # Overlay some traces for context (sample up to 100 trials)\n",
    "        trial_ids = gdf[\"trial\"].unique()\n",
    "        rng = np.random.default_rng(123)\n",
    "        sample = rng.choice(trial_ids, size=min(100, len(trial_ids)), replace=False)\n",
    "        for tr in sample:\n",
    "            tr_df = gdf[gdf[\"trial\"] == tr]\n",
    "            ax.plot(tr_df[\"time\"], tr_df[\"X\"], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.1, linewidth=0.8)\n",
    "\n",
    "        ax.plot(t, mean, color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), linewidth=2, label=\"mean\")\n",
    "        ax.set_title(f\"{group.capitalize()} adoption\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"X(t)\")\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "        # Final X(T) histogram\n",
    "        t_max = int(gdf[\"time\"].max())\n",
    "        final_vals = gdf[gdf[\"time\"] == t_max].groupby(\"trial\")[\"X\"].mean().to_numpy()\n",
    "        axes[1, j].hist(final_vals, bins=20, color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=0.8)\n",
    "        axes[1, j].set_title(f\"{group.capitalize()} final X(T)\")\n",
    "        axes[1, j].set_xlabel(\"X(T)\")\n",
    "        axes[1, j].set_ylabel(\"Count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_intervention_fanchart.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_spaghetti(traces_df: pd.DataFrame, *, max_traces: int = 100, alpha: float = 0.15, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Spaghetti plot from traces DF for baseline vs subsidy.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 4.5), constrained_layout=True)\n",
    "    rng = np.random.default_rng(123)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        trial_ids = gdf[\"trial\"].unique()\n",
    "        sample = rng.choice(trial_ids, size=min(max_traces, len(trial_ids)), replace=False)\n",
    "        ax = axes[j]\n",
    "        for tr in sample:\n",
    "            tr_df = gdf[gdf[\"trial\"] == tr]\n",
    "            ax.plot(tr_df[\"time\"], tr_df[\"X\"], color=(\"steelblue\" if group == \"baseline\" else \"darkorange\"), alpha=alpha, linewidth=0.8)\n",
    "        ax.set_title(f\"{group.capitalize()} traces\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"X(t)\")\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_spaghetti.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_density(traces_df: pd.DataFrame, *, x_bins: int = 50, time_bins: Optional[int] = None, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Time-evolving density plot (2D histogram) from traces DF.\"\"\"\n",
    "    groups = [\"baseline\", \"subsidy\"]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), constrained_layout=True)\n",
    "\n",
    "    for j, group in enumerate(groups):\n",
    "        gdf = traces_df[traces_df[\"group\"] == group]\n",
    "        T = int(gdf[\"time\"].max()) + 1\n",
    "        if time_bins is None:\n",
    "            bins_time = T\n",
    "        else:\n",
    "            bins_time = time_bins\n",
    "        hb = axes[j].hist2d(gdf[\"time\"].to_numpy(), gdf[\"X\"].to_numpy(), bins=[bins_time, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "        axes[j].set_title(f\"{group.capitalize()} density: time vs X(t)\")\n",
    "        axes[j].set_xlabel(\"Time\")\n",
    "        axes[j].set_ylabel(\"X(t)\")\n",
    "        fig.colorbar(hb[3], ax=axes[j], label=\"count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_density.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_ratio_sweep(sweep_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot X* vs ratio from a DataFrame with columns ['ratio','X_mean'].\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(sweep_df[\"ratio\"], sweep_df[\"X_mean\"], color=\"C0\", lw=2)\n",
    "    ax.set_xlabel(\"a_I / b (ratio)\")\n",
    "    ax.set_ylabel(\"Final adoption X*\")\n",
    "    ax.set_title(\"X* vs ratio\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_ratio_sweep.png\")\n",
    "    fig.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def plot_phase_plot(phase_df: pd.DataFrame, out_path: Optional[str] = None) -> str:\n",
    "    \"\"\"Plot heatmap from tidy DataFrame with columns ['X0','ratio','X_final'].\"\"\"\n",
    "    # Pivot to matrix for imshow\n",
    "    pivot = phase_df.pivot(index=\"ratio\", columns=\"X0\", values=\"X_final\").sort_index().sort_index(axis=1)\n",
    "    ratios = pivot.index.to_numpy()\n",
    "    X0s = pivot.columns.to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    im = plt.imshow(\n",
    "        pivot.to_numpy(),\n",
    "        origin=\"lower\",\n",
    "        extent=[X0s[0], X0s[-1], ratios[0], ratios[-1]],\n",
    "        aspect=\"auto\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Final adopters X*\")\n",
    "    plt.xlabel(\"X0 (initial adoption)\")\n",
    "    plt.ylabel(\"a_I / b (initial payoff ratio)\")\n",
    "    plt.title(\"Network phase plot: X* over X0 and a_I/b\")\n",
    "\n",
    "    # Overlay threshold X = 1/ratio\n",
    "    X_thresh = 1.0 / ratios\n",
    "    X_thresh_clipped = np.clip(X_thresh, 0.0, 1.0)\n",
    "    plt.plot(X_thresh_clipped, ratios, color=\"white\", linestyle=\"--\", linewidth=1.5, label=\"X = b / a_I (initial)\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = _default_plot_path(\"ev_phase_plot.png\")\n",
    "    plt.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de861a7",
   "metadata": {},
   "source": [
    "## start ev_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "663a40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Policy factories\n",
    "# -----------------------------\n",
    "\n",
    "def policy_subsidy_factory(start: int, end: int, delta_a0: float = 0.3, delta_beta_I: float = 0.0) -> Callable:\n",
    "    \"\"\"Create a policy that temporarily boosts coordination payoffs.\n",
    "\n",
    "    Raises `a0` and/or `beta_I` during `[start, end)` and reverts after.\n",
    "    Returns a closure `policy(model, step)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def policy(model, step):\n",
    "        if not hasattr(policy, \"base_a0\"):\n",
    "            policy.base_a0 = model.a0\n",
    "        if not hasattr(policy, \"base_beta_I\"):\n",
    "            policy.base_beta_I = model.beta_I\n",
    "\n",
    "        if start <= step < end:\n",
    "            model.a0 = policy.base_a0 + delta_a0\n",
    "            model.beta_I = policy.base_beta_I + delta_beta_I\n",
    "        else:\n",
    "            model.a0 = policy.base_a0\n",
    "            model.beta_I = policy.base_beta_I\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_infrastructure_boost_factory(start: int, boost: float = 0.2, once: bool = True) -> Callable:\n",
    "    \"\"\"Create a policy that injects infrastructure at a specific step.\"\"\"\n",
    "\n",
    "    def policy(model, step):\n",
    "        if step < start:\n",
    "            return\n",
    "        if once:\n",
    "            if not hasattr(policy, \"done\"):\n",
    "                model.infrastructure = float(np.clip(model.infrastructure + boost, 0.0, 1.0))\n",
    "                policy.done = True\n",
    "        else:\n",
    "            model.infrastructure = float(np.clip(model.infrastructure + boost, 0.0, 1.0))\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5f33b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Trial runner\n",
    "# -----------------------------\n",
    "\n",
    "def run_timeseries_trial(\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    seed: Optional[int] = None,\n",
    "    policy: Optional[Callable] = None,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"Run a single simulation and return X(t), I(t), and the model dataframe.\"\"\"\n",
    "\n",
    "    scenario = {\n",
    "        # Either provide `ratio` to pin the initial a_I/b, or explicit `a0`.\n",
    "        # Defaults here mirror the classroom-friendly values.\n",
    "        # If `ratio` is present, we compute `a0 = ratio*b - beta_I*I0`.\n",
    "        \"a0\": 2.0,\n",
    "        \"ratio\": None,\n",
    "        \"beta_I\": 3.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.1,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"random\",\n",
    "        \"n_nodes\": 100,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"collect\": True,\n",
    "        \"X0_frac\": 0.0,\n",
    "        \"init_method\": \"random\",\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    # Compute a0 from ratio if provided to preserve initial payoff ratio\n",
    "    a0_for_model = scenario[\"a0\"]\n",
    "    if scenario.get(\"ratio\") is not None:\n",
    "        a0_for_model = float(scenario[\"ratio\"]) * float(scenario[\"b\"]) - float(scenario[\"beta_I\"]) * float(scenario[\"I0\"])\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        a0=a0_for_model,\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        I0=scenario[\"I0\"],\n",
    "        seed=seed,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        collect=True,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    if scenario.get(\"X0_frac\", 0.0) > 0.0:\n",
    "        set_initial_adopters(\n",
    "            model,\n",
    "            scenario[\"X0_frac\"],\n",
    "            method=scenario.get(\"init_method\", \"random\"),\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "    for t in range(T):\n",
    "        if policy is not None:\n",
    "            policy(model, t)\n",
    "        model.step()\n",
    "\n",
    "    df = model.datacollector.get_model_vars_dataframe().copy()\n",
    "    return df[\"X\"].to_numpy(), df[\"I\"].to_numpy(), df\n",
    "\n",
    "\n",
    "def _timeseries_trial_worker(args_dict: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Worker for parallel trials that reconstructs closures for policies.\"\"\"\n",
    "    T = args_dict[\"T\"]\n",
    "    scenario_kwargs = args_dict.get(\"scenario_kwargs\", {})\n",
    "    seed = args_dict.get(\"seed\", None)\n",
    "    policy_spec = args_dict.get(\"policy\", None)\n",
    "    strategy_choice_func = args_dict.get(\"strategy_choice_func\", \"imitate\")\n",
    "    tau = args_dict.get(\"tau\", 1.0)\n",
    "\n",
    "    policy = None\n",
    "    if isinstance(policy_spec, dict):\n",
    "        ptype = policy_spec.get(\"type\")\n",
    "        if ptype == \"subsidy\":\n",
    "            policy = policy_subsidy_factory(**policy_spec[\"params\"])\n",
    "        elif ptype == \"infrastructure\":\n",
    "            policy = policy_infrastructure_boost_factory(**policy_spec[\"params\"])\n",
    "\n",
    "    X, I, _df = run_timeseries_trial(\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario_kwargs,\n",
    "        seed=seed,\n",
    "        policy=policy,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    return X, I\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "853302db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Experiment: Intervention trials + plotting\n",
    "# -----------------------------\n",
    "\n",
    "def collect_intervention_trials(\n",
    "    n_trials: int = 10,\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    subsidy_params: Optional[Dict] = None,\n",
    "    max_workers: int = 1,\n",
    "    seed_base: int = 42,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[List[np.ndarray], List[np.ndarray], List[np.ndarray], List[np.ndarray], pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Run baseline and subsidy trials; return raw trajectories and summary dataframes.\"\"\"\n",
    "\n",
    "    scenario = scenario_kwargs or {}\n",
    "    subsidy = subsidy_params or {\"start\": 30, \"end\": 80, \"delta_a0\": 0.3, \"delta_beta_I\": 0.0}\n",
    "\n",
    "    baseline_args = []\n",
    "    subsidy_args = []\n",
    "    for i in range(n_trials):\n",
    "        seed = seed_base + i\n",
    "        baseline_args.append(\n",
    "            {\n",
    "                \"T\": T,\n",
    "                \"scenario_kwargs\": scenario,\n",
    "                \"seed\": seed,\n",
    "                \"policy\": None,\n",
    "                \"strategy_choice_func\": strategy_choice_func,\n",
    "                \"tau\": tau,\n",
    "            }\n",
    "        )\n",
    "        subsidy_args.append(\n",
    "            {\n",
    "                \"T\": T,\n",
    "                \"scenario_kwargs\": scenario,\n",
    "                \"seed\": seed,\n",
    "                \"policy\": {\"type\": \"subsidy\", \"params\": subsidy},\n",
    "                \"strategy_choice_func\": strategy_choice_func,\n",
    "                \"tau\": tau,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    baseline_X, baseline_I = [], []\n",
    "    subsidy_X, subsidy_I = [], []\n",
    "\n",
    "    # Run sequentially or concurrently\n",
    "    Executor = ThreadPoolExecutor if max_workers == 1 else ProcessPoolExecutor\n",
    "    with Executor(max_workers=max_workers) as ex:\n",
    "        baseline_futs = [ex.submit(_timeseries_trial_worker, args) for args in baseline_args]\n",
    "        subsidy_futs = [ex.submit(_timeseries_trial_worker, args) for args in subsidy_args]\n",
    "        for fut in as_completed(baseline_futs):\n",
    "            X, I = fut.result()\n",
    "            baseline_X.append(X)\n",
    "            baseline_I.append(I)\n",
    "        for fut in as_completed(subsidy_futs):\n",
    "            X, I = fut.result()\n",
    "            subsidy_X.append(X)\n",
    "            subsidy_I.append(I)\n",
    "\n",
    "    # Align order by seed (as_completed may scramble)\n",
    "    baseline_X = sorted(baseline_X, key=lambda arr: tuple(arr))\n",
    "    subsidy_X = sorted(subsidy_X, key=lambda arr: tuple(arr))\n",
    "\n",
    "    # Summary stats\n",
    "    def summarize(X_list: List[np.ndarray]) -> pd.DataFrame:\n",
    "        mat = np.vstack(X_list)\n",
    "        df = pd.DataFrame({\n",
    "            \"X_mean\": mat.mean(axis=0),\n",
    "            \"X_med\": np.median(mat, axis=0),\n",
    "            \"X_q10\": np.quantile(mat, 0.10, axis=0),\n",
    "            \"X_q25\": np.quantile(mat, 0.25, axis=0),\n",
    "            \"X_q75\": np.quantile(mat, 0.75, axis=0),\n",
    "            \"X_q90\": np.quantile(mat, 0.90, axis=0),\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    baseline_df = summarize(baseline_X)\n",
    "    subsidy_df = summarize(subsidy_X)\n",
    "\n",
    "    return baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df, subsidy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35582615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def traces_to_long_df(baseline_X: List[np.ndarray], subsidy_X: List[np.ndarray]) -> pd.DataFrame:\n",
    "    \"\"\"Convert trajectory lists to a tidy DataFrame: [group, trial, time, X].\"\"\"\n",
    "    rows = []\n",
    "    for trial, X in enumerate(baseline_X):\n",
    "        for t, x in enumerate(X):\n",
    "            rows.append((\"baseline\", trial, t, float(x)))\n",
    "    for trial, X in enumerate(subsidy_X):\n",
    "        for t, x in enumerate(X):\n",
    "            rows.append((\"subsidy\", trial, t, float(x)))\n",
    "    return pd.DataFrame(rows, columns=[\"group\", \"trial\", \"time\", \"X\"])\n",
    "\n",
    "\n",
    "def ratio_sweep_df(\n",
    "    X0_frac: float = 0.40,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute X* vs ratio and return as a DataFrame.\"\"\"\n",
    "    scenario = {\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_ratio(\n",
    "        X0_frac,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame({\"ratio\": ratio_values, \"X_mean\": X_means})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac6f05b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def phase_sweep_df(\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    "    X0_values: Optional[np.ndarray] = None,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    T: int = 250,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute tidy DataFrame of X* over (X0, ratio).\"\"\"\n",
    "    if X0_values is None:\n",
    "        X0_values = np.linspace(0.0, 1.0, 21)\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    scenario = {\n",
    "        \"I0\": 0.05,\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    X_final = phase_sweep_X0_vs_ratio(\n",
    "        X0_values,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        max_workers=max_workers or 1,\n",
    "        backend=backend,\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for i, X0 in enumerate(X0_values):\n",
    "        for j, ratio in enumerate(ratio_values):\n",
    "            rows.append((float(X0), float(ratio), float(X_final[j, i])))\n",
    "    return pd.DataFrame(rows, columns=[\"X0\", \"ratio\", \"X_final\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9cf31a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_intervention_fanchart(\n",
    "    baseline_X: List[np.ndarray],\n",
    "    subsidy_X: List[np.ndarray],\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Plot fan charts for baseline and subsidy trials and save to file.\n",
    "\n",
    "    Returns the file path to the saved image.\n",
    "    \"\"\"\n",
    "    T = len(baseline_X[0]) if baseline_X else 0\n",
    "    t = np.arange(T)\n",
    "\n",
    "    def quantiles(X_list: List[np.ndarray]):\n",
    "        mat = np.vstack(X_list)\n",
    "        return {\n",
    "            \"mean\": mat.mean(axis=0),\n",
    "            \"q10\": np.quantile(mat, 0.10, axis=0),\n",
    "            \"q25\": np.quantile(mat, 0.25, axis=0),\n",
    "            \"q75\": np.quantile(mat, 0.75, axis=0),\n",
    "            \"q90\": np.quantile(mat, 0.90, axis=0),\n",
    "            \"final\": mat[:, -1],\n",
    "        }\n",
    "\n",
    "    bq = quantiles(baseline_X)\n",
    "    sq = quantiles(subsidy_X)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(11, 8), constrained_layout=True)\n",
    "\n",
    "    # Baseline fan chart\n",
    "    ax = axes[0, 0]\n",
    "    ax.fill_between(t, bq[\"q10\"], bq[\"q90\"], color=\"steelblue\", alpha=0.15, label=\"10–90%\")\n",
    "    ax.fill_between(t, bq[\"q25\"], bq[\"q75\"], color=\"steelblue\", alpha=0.30, label=\"25–75%\")\n",
    "    for X in baseline_X:\n",
    "        ax.plot(t, X, color=\"steelblue\", alpha=0.10, linewidth=1)\n",
    "    ax.plot(t, bq[\"mean\"], color=\"steelblue\", linewidth=2, label=\"mean\")\n",
    "    ax.set_title(\"Baseline adoption\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"X(t)\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Subsidy fan chart\n",
    "    ax = axes[0, 1]\n",
    "    ax.fill_between(t, sq[\"q10\"], sq[\"q90\"], color=\"darkorange\", alpha=0.15, label=\"10–90%\")\n",
    "    ax.fill_between(t, sq[\"q25\"], sq[\"q75\"], color=\"darkorange\", alpha=0.30, label=\"25–75%\")\n",
    "    for X in subsidy_X:\n",
    "        ax.plot(t, X, color=\"darkorange\", alpha=0.10, linewidth=1)\n",
    "    ax.plot(t, sq[\"mean\"], color=\"darkorange\", linewidth=2, label=\"mean\")\n",
    "    ax.set_title(\"Subsidy adoption\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"X(t)\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    # Histograms of final X(T)\n",
    "    axes[1, 0].hist(bq[\"final\"], bins=20, color=\"steelblue\", alpha=0.8)\n",
    "    axes[1, 0].set_title(\"Baseline final adoption X(T)\")\n",
    "    axes[1, 0].set_xlabel(\"X(T)\")\n",
    "    axes[1, 0].set_ylabel(\"Count\")\n",
    "\n",
    "    axes[1, 1].hist(sq[\"final\"], bins=20, color=\"darkorange\", alpha=0.8)\n",
    "    axes[1, 1].set_title(\"Subsidy final adoption X(T)\")\n",
    "    axes[1, 1].set_xlabel(\"X(T)\")\n",
    "    axes[1, 1].set_ylabel(\"Count\")\n",
    "\n",
    "    # Save figure\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_intervention_fanchart.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b1dd9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_spaghetti_traces(\n",
    "    baseline_X: List[np.ndarray],\n",
    "    subsidy_X: List[np.ndarray],\n",
    "    *,\n",
    "    max_traces: int = 100,\n",
    "    alpha: float = 0.15,\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Plot raw trajectories as thin, transparent lines for baseline/subsidy.\n",
    "\n",
    "    Shows bifurcation visually: many lines diverging toward 0 or 1 over time.\n",
    "    \"\"\"\n",
    "    # Select random subset for visual clarity\n",
    "    rng = np.random.default_rng(123)\n",
    "    def subset(trajs: List[np.ndarray]) -> List[np.ndarray]:\n",
    "        if len(trajs) <= max_traces:\n",
    "            return trajs\n",
    "        idx = rng.choice(len(trajs), size=max_traces, replace=False)\n",
    "        return [trajs[i] for i in idx]\n",
    "\n",
    "    b_sub = subset(baseline_X)\n",
    "    s_sub = subset(subsidy_X)\n",
    "\n",
    "    T = len(b_sub[0]) if b_sub else 0\n",
    "    t = np.arange(T)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 4.5), constrained_layout=True)\n",
    "\n",
    "    ax = axes[0]\n",
    "    for X in b_sub:\n",
    "        ax.plot(t, X, color=\"steelblue\", alpha=alpha, linewidth=0.8)\n",
    "    ax.set_title(\"Baseline traces\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"X(t)\")\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    ax = axes[1]\n",
    "    for X in s_sub:\n",
    "        ax.plot(t, X, color=\"darkorange\", alpha=alpha, linewidth=0.8)\n",
    "    ax.set_title(\"Subsidy traces\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"X(t)\")\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_spaghetti.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f08fa3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_time_evolving_density(\n",
    "    baseline_X: List[np.ndarray],\n",
    "    subsidy_X: List[np.ndarray],\n",
    "    *,\n",
    "    x_bins: int = 50,\n",
    "    time_bins: Optional[int] = None,\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Plot 2D histograms of (time, X) densities for baseline and subsidy.\n",
    "\n",
    "    X-axis: time, Y-axis: adoption X(t), Color: frequency/density of passes.\n",
    "    \"\"\"\n",
    "    if not baseline_X or not subsidy_X:\n",
    "        raise ValueError(\"Need non-empty baseline and subsidy trajectories\")\n",
    "\n",
    "    T = len(baseline_X[0])\n",
    "    if time_bins is None:\n",
    "        time_bins = T\n",
    "\n",
    "    # Flatten (t, X) points across all trials\n",
    "    def flatten_points(trajs: List[np.ndarray]):\n",
    "        t = np.arange(T)\n",
    "        t_all = np.repeat(t, len(trajs))\n",
    "        x_all = np.hstack(trajs)\n",
    "        return t_all, x_all\n",
    "\n",
    "    bt, bx = flatten_points(baseline_X)\n",
    "    st, sx = flatten_points(subsidy_X)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4.8), constrained_layout=True)\n",
    "\n",
    "    hb = axes[0].hist2d(bt, bx, bins=[time_bins, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "    axes[0].set_title(\"Baseline density: time vs X(t)\")\n",
    "    axes[0].set_xlabel(\"Time\")\n",
    "    axes[0].set_ylabel(\"X(t)\")\n",
    "    fig.colorbar(hb[3], ax=axes[0], label=\"count\")\n",
    "\n",
    "    hs = axes[1].hist2d(st, sx, bins=[time_bins, x_bins], range=[[0, T - 1], [0.0, 1.0]], cmap=\"magma\")\n",
    "    axes[1].set_title(\"Subsidy density: time vs X(t)\")\n",
    "    axes[1].set_xlabel(\"Time\")\n",
    "    axes[1].set_ylabel(\"X(t)\")\n",
    "    fig.colorbar(hs[3], ax=axes[1], label=\"count\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_density.png\")\n",
    "    fig.savefig(out_path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "195c0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_ratio_sweep_plot(\n",
    "    X0_frac: float = 0.40,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    T: int = 250,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    out_path: Optional[str] = None,\n",
    ") -> str:\n",
    "    \"\"\"Sweep ratio values and plot final adoption X* vs a_I/b for a fixed X0.\n",
    "\n",
    "    Calls the core computation helper and saves a simple line plot.\n",
    "    Returns the path to the saved image.\n",
    "    \"\"\"\n",
    "    scenario = {\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    X_means = final_mean_adoption_vs_ratio(\n",
    "        X0_frac,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(ratio_values, X_means, color=\"tabblue\" if hasattr(plt, \"tabblue\") else \"C0\", lw=2)\n",
    "    ax.set_xlabel(\"a_I / b (ratio)\")\n",
    "    ax.set_ylabel(\"Final adoption X*\")\n",
    "    ax.set_title(f\"X* vs ratio for X0={X0_frac:.2f}\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.25)\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_ratio_sweep.png\")\n",
    "    fig.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    return out_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d73bf83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_phase_plot_X0_vs_ratio_network(\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    "    X0_values: Optional[np.ndarray] = None,\n",
    "    ratio_values: Optional[np.ndarray] = None,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    T: int = 250,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    out_path: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Produce a heatmap of X* over (X0, a_I/b) using core sweep helper.\n",
    "\n",
    "    Saves a figure similar to the original model script and returns the path.\n",
    "    \"\"\"\n",
    "    # Defaults aligned with the original phase plot\n",
    "    if X0_values is None:\n",
    "        X0_values = np.linspace(0.0, 1.0, 21)\n",
    "    if ratio_values is None:\n",
    "        ratio_values = np.linspace(0.8, 3.5, 41)\n",
    "\n",
    "    scenario = {\n",
    "        \"I0\": 0.05,\n",
    "        \"beta_I\": 2.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.05,\n",
    "        \"network_type\": \"BA\",\n",
    "        \"n_nodes\": 120,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    X_final = phase_sweep_X0_vs_ratio(\n",
    "        X0_values,\n",
    "        ratio_values,\n",
    "        I0=scenario[\"I0\"],\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        T=T,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        batch_size=batch_size,\n",
    "        init_noise_I=init_noise_I,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "        max_workers=max_workers or 1,\n",
    "        backend=backend,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    im = plt.imshow(\n",
    "        X_final,\n",
    "        origin=\"lower\",\n",
    "        extent=[X0_values[0], X0_values[-1], ratio_values[0], ratio_values[-1]],\n",
    "        aspect=\"auto\",\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    plt.colorbar(im, label=\"Final adopters X*\")\n",
    "    plt.xlabel(\"X0 (initial adoption)\")\n",
    "    plt.ylabel(\"a_I / b (initial payoff ratio)\")\n",
    "    plt.title(\"Network phase plot: X* over X0 and a_I/b\")\n",
    "\n",
    "    # Overlay initial threshold X = b/a_I => X = 1/ratio\n",
    "    X_thresh = 1.0 / ratio_values\n",
    "    X_thresh_clipped = np.clip(X_thresh, 0.0, 1.0)\n",
    "    plt.plot(\n",
    "        X_thresh_clipped,\n",
    "        ratio_values,\n",
    "        color=\"white\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=1.5,\n",
    "        label=\"X = b / a_I (initial)\",\n",
    "    )\n",
    "\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = os.path.join(os.getcwd(), \"ev_phase_plot.png\")\n",
    "    plt.savefig(out_path, dpi=140, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "adbc99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_intervention_example(\n",
    "    n_trials: int = 10,\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    subsidy_params: Optional[Dict] = None,\n",
    "    max_workers: int = 1,\n",
    "    seed_base: int = 42,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, str]:\n",
    "    \"\"\"Convenience: collect trials, plot, and return summary + image path.\"\"\"\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df, subsidy_df = collect_intervention_trials(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario_kwargs,\n",
    "        subsidy_params=subsidy_params,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    # Use DataFrame-based plotting to ensure outputs go to plots/\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    img_path = plot_fanchart(traces_df)\n",
    "    return baseline_df, subsidy_df, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48490ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline DF shape: (200, 6)\n",
      "Subsidy DF shape: (200, 6)\n",
      "Saved image: /Users/lenavanleeuwen/Desktop/Model-based-decision-making/Assignment_3_MBD/plots/ev_intervention_fanchart.png\n",
      "Baseline final X_mean: 0.6474444444444444\n",
      "Subsidy  final X_mean: 0.6789999999999999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# CLI Entrypoint\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    # Defaults aligned with original ev_stag_mesa_model.run_intervention_example\n",
    "    n_trials = 30  # use fewer than 500 for speed while keeping shape\n",
    "    T = 200\n",
    "    strategy_choice_func = \"imitate\"\n",
    "    tau = 1.0\n",
    "    max_workers = 1\n",
    "    seed_base = 100\n",
    "\n",
    "    scenario = dict(\n",
    "        # Preserve initial ratio by computing a0 from ratio, matching the original\n",
    "        ratio=2.3,\n",
    "        beta_I=2.0,\n",
    "        b=1.0,\n",
    "        g_I=0.10,\n",
    "        I0=0.05,\n",
    "        network_type=\"BA\",\n",
    "        n_nodes=300,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        X0_frac=0.40,\n",
    "        init_method=\"random\",\n",
    "        # ER-specific `p` ignored for BA but kept for completeness\n",
    "        p=0.05,\n",
    "    )\n",
    "    subsidy = dict(start=10, end=60, delta_a0=0.4, delta_beta_I=0.0)\n",
    "\n",
    "    baseline_df, subsidy_df, img_path = run_intervention_example(\n",
    "        n_trials=n_trials,\n",
    "        T=T,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    print(\"Baseline DF shape:\", baseline_df.shape)\n",
    "    print(\"Subsidy DF shape:\", subsidy_df.shape)\n",
    "    print(\"Saved image:\", img_path)\n",
    "    print(\"Baseline final X_mean:\", float(baseline_df[\"X_mean\"].iloc[-1]))\n",
    "    print(\"Subsidy  final X_mean:\", float(subsidy_df[\"X_mean\"].iloc[-1]))\n",
    "\n",
    "    # Also run the phase plot of X* over (X0, a_I/b) and save it\n",
    "    phase_df = phase_sweep_df(\n",
    "        max_workers=1,\n",
    "        backend=\"thread\",\n",
    "        X0_values=np.linspace(0.0, 1.0, 21),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        batch_size=8,\n",
    "        T=200,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    phase_path = plot_phase_plot(phase_df)\n",
    "    print(\"Saved phase plot:\", phase_path)\n",
    "\n",
    "    # Spaghetti and time-evolving density plots\n",
    "    # Use a larger trial count for clearer trace/density visuals\n",
    "    n_trials_spaghetti = 100\n",
    "    T_spaghetti = 200\n",
    "\n",
    "    baseline_X, baseline_I, subsidy_X, subsidy_I, baseline_df2, subsidy_df2 = collect_intervention_trials(\n",
    "        n_trials=n_trials_spaghetti,\n",
    "        T=T_spaghetti,\n",
    "        scenario_kwargs=scenario,\n",
    "        subsidy_params=subsidy,\n",
    "        max_workers=max_workers,\n",
    "        seed_base=seed_base,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "    traces_df = traces_to_long_df(baseline_X, subsidy_X)\n",
    "    spaghetti_path = plot_spaghetti(traces_df, max_traces=100, alpha=0.15)\n",
    "    print(\"Saved spaghetti plot:\", spaghetti_path)\n",
    "\n",
    "    density_path = plot_density(traces_df, x_bins=50, time_bins=T_spaghetti)\n",
    "    print(\"Saved time-evolving density plot:\", density_path)\n",
    "\n",
    "    # Ratio sweep computed to DF then plotted\n",
    "    sweep_df = ratio_sweep_df(\n",
    "        X0_frac=scenario.get(\"X0_frac\", 0.40),\n",
    "        ratio_values=np.linspace(0.8, 3.5, 31),\n",
    "        scenario_kwargs=scenario,\n",
    "        T=200,\n",
    "        batch_size=8,\n",
    "        strategy_choice_func=\"logit\",\n",
    "        tau=1.0,\n",
    "    )\n",
    "    sweep_path = plot_ratio_sweep(sweep_df)\n",
    "    print(\"Saved ratio sweep plot:\", sweep_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd40a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
